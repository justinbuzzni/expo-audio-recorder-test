This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: node_modules/
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
__tests__/
  AudioRecorder.simple.test.js
  AudioRecorder.test.js
.claude/
  settings.local.json
.expo/
  devices.json
  README.md
components/
  AudioRecorder.js
App.js
app.json
jest-setup.js
jest.config.js
package.json
test-audio.js
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="__tests__/AudioRecorder.simple.test.js">
// Simple unit test without React Native components
const { Audio } = require('expo-av');
const FileSystem = require('expo-file-system');

// Mock modules
jest.mock('expo-av');
jest.mock('expo-file-system');

describe('Audio Recording Logic Tests', () => {
  beforeEach(() => {
    jest.clearAllMocks();
  });

  test('Audio permissions can be requested', async () => {
    Audio.requestPermissionsAsync = jest.fn().mockResolvedValue({ status: 'granted' });
    
    const result = await Audio.requestPermissionsAsync();
    
    expect(result.status).toBe('granted');
    expect(Audio.requestPermissionsAsync).toHaveBeenCalledTimes(1);
  });

  test('Audio mode can be set', async () => {
    Audio.setAudioModeAsync = jest.fn().mockResolvedValue();
    
    await Audio.setAudioModeAsync({
      allowsRecordingIOS: true,
      playsInSilentModeIOS: true,
    });
    
    expect(Audio.setAudioModeAsync).toHaveBeenCalledWith({
      allowsRecordingIOS: true,
      playsInSilentModeIOS: true,
    });
  });

  test('Recording can be created', async () => {
    const mockRecording = {
      stopAndUnloadAsync: jest.fn().mockResolvedValue(),
      getURI: jest.fn().mockReturnValue('mock://recording.m4a'),
    };
    
    Audio.Recording = {
      createAsync: jest.fn().mockResolvedValue({ recording: mockRecording }),
    };
    Audio.RecordingOptionsPresets = { HIGH_QUALITY: {} };
    
    const { recording } = await Audio.Recording.createAsync(Audio.RecordingOptionsPresets.HIGH_QUALITY);
    
    expect(Audio.Recording.createAsync).toHaveBeenCalledWith(Audio.RecordingOptionsPresets.HIGH_QUALITY);
    expect(recording).toBeDefined();
    expect(recording.getURI()).toBe('mock://recording.m4a');
  });

  test('File can be moved', async () => {
    FileSystem.documentDirectory = 'file:///mock/document/directory/';
    FileSystem.moveAsync = jest.fn().mockResolvedValue();
    
    const from = 'mock://recording.m4a';
    const to = FileSystem.documentDirectory + 'recording_0_123456789.m4a';
    
    await FileSystem.moveAsync({ from, to });
    
    expect(FileSystem.moveAsync).toHaveBeenCalledWith({ from, to });
  });

  test('5-second interval creates multiple segments', async () => {
    jest.useFakeTimers();
    
    let segmentCount = 0;
    const interval = setInterval(() => {
      segmentCount++;
    }, 5000);
    
    // Fast forward 15 seconds
    jest.advanceTimersByTime(15000);
    
    expect(segmentCount).toBe(3); // 3 segments in 15 seconds
    
    clearInterval(interval);
    jest.useRealTimers();
  });

  test('Permission denied scenario', async () => {
    Audio.requestPermissionsAsync = jest.fn().mockResolvedValue({ status: 'denied' });
    
    const result = await Audio.requestPermissionsAsync();
    
    expect(result.status).toBe('denied');
  });

  test('Recording stop and unload', async () => {
    const mockRecording = {
      stopAndUnloadAsync: jest.fn().mockResolvedValue(),
      getURI: jest.fn().mockReturnValue('mock://recording.m4a'),
    };
    
    await mockRecording.stopAndUnloadAsync();
    const uri = mockRecording.getURI();
    
    expect(mockRecording.stopAndUnloadAsync).toHaveBeenCalled();
    expect(uri).toBe('mock://recording.m4a');
  });
});

// Integration test for the recording flow
describe('Recording Flow Integration', () => {
  test('Complete recording flow', async () => {
    // Setup mocks
    Audio.requestPermissionsAsync = jest.fn().mockResolvedValue({ status: 'granted' });
    Audio.setAudioModeAsync = jest.fn().mockResolvedValue();
    
    const mockRecording = {
      stopAndUnloadAsync: jest.fn().mockResolvedValue(),
      getURI: jest.fn().mockReturnValue('mock://recording.m4a'),
    };
    
    Audio.Recording = {
      createAsync: jest.fn().mockResolvedValue({ recording: mockRecording }),
    };
    Audio.RecordingOptionsPresets = { HIGH_QUALITY: {} };
    
    FileSystem.documentDirectory = 'file:///mock/document/directory/';
    FileSystem.moveAsync = jest.fn().mockResolvedValue();
    
    // Execute flow
    // 1. Request permissions
    const permission = await Audio.requestPermissionsAsync();
    expect(permission.status).toBe('granted');
    
    // 2. Set audio mode
    await Audio.setAudioModeAsync({
      allowsRecordingIOS: true,
      playsInSilentModeIOS: true,
    });
    
    // 3. Create recording
    const { recording } = await Audio.Recording.createAsync(Audio.RecordingOptionsPresets.HIGH_QUALITY);
    
    // 4. Stop recording
    await recording.stopAndUnloadAsync();
    
    // 5. Get URI and move file
    const uri = recording.getURI();
    const newUri = FileSystem.documentDirectory + 'recording_0_123456789.m4a';
    await FileSystem.moveAsync({ from: uri, to: newUri });
    
    // Verify all steps
    expect(Audio.requestPermissionsAsync).toHaveBeenCalled();
    expect(Audio.setAudioModeAsync).toHaveBeenCalled();
    expect(Audio.Recording.createAsync).toHaveBeenCalled();
    expect(recording.stopAndUnloadAsync).toHaveBeenCalled();
    expect(FileSystem.moveAsync).toHaveBeenCalledWith({
      from: 'mock://recording.m4a',
      to: expect.stringContaining('recording_0_')
    });
  });
});
</file>

<file path="__tests__/AudioRecorder.test.js">
import React from 'react';
import { render, fireEvent, waitFor, act } from '@testing-library/react-native';
import { Alert } from 'react-native';
import { Audio } from 'expo-av';
import * as FileSystem from 'expo-file-system';
import AudioRecorder from '../components/AudioRecorder';

// Mock the modules
jest.mock('expo-av');
jest.mock('expo-file-system');
jest.spyOn(Alert, 'alert');

describe('AudioRecorder', () => {
  let mockRecording;
  let mockSound;

  beforeEach(() => {
    jest.clearAllMocks();
    jest.useFakeTimers();

    // Mock recording
    mockRecording = {
      stopAndUnloadAsync: jest.fn(),
      getURI: jest.fn().mockReturnValue('mock://recording.m4a'),
    };

    // Mock sound
    mockSound = {
      unloadAsync: jest.fn(),
      setOnPlaybackStatusUpdate: jest.fn(),
    };

    // Setup default mocks
    Audio.requestPermissionsAsync.mockResolvedValue({ status: 'granted' });
    Audio.setAudioModeAsync.mockResolvedValue();
    Audio.Recording.createAsync.mockResolvedValue({ recording: mockRecording });
    Audio.Sound.createAsync.mockResolvedValue({ sound: mockSound });
    FileSystem.moveAsync.mockResolvedValue();
  });

  afterEach(() => {
    jest.runOnlyPendingTimers();
    jest.useRealTimers();
  });

  it('renders correctly', () => {
    const { getByText, getByTestId } = render(<AudioRecorder />);
    
    expect(getByText('5초 단위 음성 녹음기')).toBeTruthy();
    expect(getByTestId('record-button')).toBeTruthy();
    expect(getByText('녹음 시작')).toBeTruthy();
  });

  it('requests permissions before recording', async () => {
    const { getByTestId } = render(<AudioRecorder />);
    const recordButton = getByTestId('record-button');

    await act(async () => {
      fireEvent.press(recordButton);
    });

    expect(Audio.requestPermissionsAsync).toHaveBeenCalled();
  });

  it('shows alert when permission is denied', async () => {
    Audio.requestPermissionsAsync.mockResolvedValue({ status: 'denied' });
    
    const { getByTestId } = render(<AudioRecorder />);
    const recordButton = getByTestId('record-button');

    await act(async () => {
      fireEvent.press(recordButton);
    });

    expect(Alert.alert).toHaveBeenCalledWith('권한 필요', '마이크 권한이 필요합니다.');
  });

  it('starts recording when button is pressed', async () => {
    const { getByTestId, getByText } = render(<AudioRecorder />);
    const recordButton = getByTestId('record-button');

    await act(async () => {
      fireEvent.press(recordButton);
    });

    await waitFor(() => {
      expect(getByText('녹음 중지')).toBeTruthy();
      expect(getByTestId('recording-indicator')).toBeTruthy();
      expect(Audio.Recording.createAsync).toHaveBeenCalled();
    });
  });

  it('displays timer during recording', async () => {
    const { getByTestId } = render(<AudioRecorder />);
    const recordButton = getByTestId('record-button');

    await act(async () => {
      fireEvent.press(recordButton);
    });

    await waitFor(() => {
      expect(getByTestId('timer-display')).toBeTruthy();
      expect(getByTestId('timer-display').props.children).toBe('00:00');
    });

    // Advance timer by 5 seconds
    act(() => {
      jest.advanceTimersByTime(5000);
    });

    await waitFor(() => {
      expect(getByTestId('timer-display').props.children).toBe('00:05');
    });
  });

  it('shows progress bar for 5-second segments', async () => {
    const { getByTestId, getByText } = render(<AudioRecorder />);
    const recordButton = getByTestId('record-button');

    await act(async () => {
      fireEvent.press(recordButton);
    });

    await waitFor(() => {
      expect(getByTestId('progress-bar')).toBeTruthy();
      expect(getByText('다음 저장까지: 5초')).toBeTruthy();
    });

    // Advance timer by 3 seconds
    act(() => {
      jest.advanceTimersByTime(3000);
    });

    await waitFor(() => {
      expect(getByText('다음 저장까지: 2초')).toBeTruthy();
    });
  });

  it('saves segments every 5 seconds', async () => {
    const { getByTestId } = render(<AudioRecorder />);
    const recordButton = getByTestId('record-button');

    await act(async () => {
      fireEvent.press(recordButton);
    });

    // Advance timer by 5 seconds to trigger segment save
    act(() => {
      jest.advanceTimersByTime(5000);
    });

    await waitFor(() => {
      expect(mockRecording.stopAndUnloadAsync).toHaveBeenCalled();
      expect(FileSystem.moveAsync).toHaveBeenCalled();
    });
  });

  it('stops recording when stop button is pressed', async () => {
    const { getByTestId } = render(<AudioRecorder />);
    const recordButton = getByTestId('record-button');

    // Start recording
    await act(async () => {
      fireEvent.press(recordButton);
    });

    // Stop recording
    await act(async () => {
      fireEvent.press(recordButton);
    });

    await waitFor(() => {
      expect(mockRecording.stopAndUnloadAsync).toHaveBeenCalled();
      expect(Alert.alert).toHaveBeenCalledWith(
        '녹음 완료',
        expect.stringContaining('개의 파일이 저장되었습니다')
      );
    });
  });

  it('displays saved segments with unique keys', async () => {
    const { getByTestId } = render(<AudioRecorder />);
    const recordButton = getByTestId('record-button');

    await act(async () => {
      fireEvent.press(recordButton);
    });

    // Advance timer to create multiple segments
    act(() => {
      jest.advanceTimersByTime(5000);
    });

    await waitFor(() => {
      const segmentsList = getByTestId('segments-list');
      const segments = segmentsList.props.children;
      
      // Check that segments have unique keys
      if (Array.isArray(segments)) {
        const keys = segments.map(segment => segment.key);
        const uniqueKeys = new Set(keys);
        expect(keys.length).toBe(uniqueKeys.size);
      }
    });
  });

  it('plays audio when segment is pressed', async () => {
    const { getByTestId } = render(<AudioRecorder />);
    const recordButton = getByTestId('record-button');

    // Record and create a segment
    await act(async () => {
      fireEvent.press(recordButton);
    });

    act(() => {
      jest.advanceTimersByTime(5000);
    });

    await act(async () => {
      fireEvent.press(recordButton);
    });

    // Find and press the first segment
    await waitFor(() => {
      const firstSegment = getByTestId('segment-0');
      fireEvent.press(firstSegment);
    });

    await waitFor(() => {
      expect(Audio.Sound.createAsync).toHaveBeenCalled();
      expect(mockSound.setOnPlaybackStatusUpdate).toHaveBeenCalled();
    });
  });

  it('stops playing when the same segment is pressed again', async () => {
    const { getByTestId } = render(<AudioRecorder />);
    const recordButton = getByTestId('record-button');

    // Record and create a segment
    await act(async () => {
      fireEvent.press(recordButton);
    });

    act(() => {
      jest.advanceTimersByTime(5000);
    });

    await act(async () => {
      fireEvent.press(recordButton);
    });

    // Press segment to play
    await waitFor(() => {
      const firstSegment = getByTestId('segment-0');
      fireEvent.press(firstSegment);
    });

    // Press same segment again to stop
    await waitFor(() => {
      const firstSegment = getByTestId('segment-0');
      fireEvent.press(firstSegment);
    });

    expect(mockSound.unloadAsync).toHaveBeenCalled();
  });

  it('handles recording errors gracefully', async () => {
    Audio.Recording.createAsync.mockRejectedValue(new Error('Recording failed'));
    
    const { getByTestId } = render(<AudioRecorder />);
    const recordButton = getByTestId('record-button');

    await act(async () => {
      fireEvent.press(recordButton);
    });

    expect(Alert.alert).toHaveBeenCalledWith('녹음 오류', '녹음을 시작할 수 없습니다.');
  });

  it('handles playback errors gracefully', async () => {
    Audio.Sound.createAsync.mockRejectedValue(new Error('Playback failed'));
    
    const { getByTestId } = render(<AudioRecorder />);
    const recordButton = getByTestId('record-button');

    // Record and create a segment
    await act(async () => {
      fireEvent.press(recordButton);
    });

    act(() => {
      jest.advanceTimersByTime(5000);
    });

    await act(async () => {
      fireEvent.press(recordButton);
    });

    // Try to play segment
    await waitFor(() => {
      const firstSegment = getByTestId('segment-0');
      fireEvent.press(firstSegment);
    });

    await waitFor(() => {
      expect(Alert.alert).toHaveBeenCalledWith('재생 오류', '오디오를 재생할 수 없습니다.');
    });
  });
});
</file>

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(npx expo install expo-av expo-file-system)",
      "Bash(npm install:*)",
      "Bash(find:*)",
      "Bash(npm uninstall:*)",
      "Bash(npx expo install:*)",
      "Bash(rm:*)",
      "Bash(npx expo start:*)",
      "WebFetch(domain:docs.expo.dev)",
      "Bash(mkdir:*)",
      "Bash(curl:*)",
      "Bash(cp:*)",
      "Bash(npm test)",
      "Bash(npm test:*)",
      "Bash(npm test:*)"
    ],
    "deny": []
  }
}
</file>

<file path=".expo/devices.json">
{
  "devices": []
}
</file>

<file path=".expo/README.md">
> Why do I have a folder named ".expo" in my project?
The ".expo" folder is created when an Expo project is started using "expo start" command.
> What do the files contain?
- "devices.json": contains information about devices that have recently opened this project. This is used to populate the "Development sessions" list in your development builds.
- "settings.json": contains the server configuration that is used to serve the application manifest.
> Should I commit the ".expo" folder?
No, you should not share the ".expo" folder. It does not contain any information that is relevant for other developers working on the project, it is specific to your machine.
Upon project creation, the ".expo" folder is already added to your ".gitignore" file.
</file>

<file path="components/AudioRecorder.js">
import React, { useState, useEffect, useRef } from 'react';
import { View, Text, TouchableOpacity, StyleSheet, Alert, ScrollView } from 'react-native';
import { Audio } from 'expo-av';
import * as FileSystem from 'expo-file-system';

const AudioRecorder = () => {
  const [recording, setRecording] = useState(null);
  const [isRecording, setIsRecording] = useState(false);
  const [recordingSegments, setRecordingSegments] = useState([]);
  const [recordingTime, setRecordingTime] = useState(0);
  const [segmentProgress, setSegmentProgress] = useState(0);
  const [playingSegmentId, setPlayingSegmentId] = useState(null);
  const [soundObject, setSoundObject] = useState(null);
  const intervalRef = useRef(null);
  const timerRef = useRef(null);
  const segmentCountRef = useRef(0);
  const isRecordingRef = useRef(false);

  useEffect(() => {
    // 컴포넌트 언마운트 시 정리 작업
    return () => {
      if (intervalRef.current) clearInterval(intervalRef.current);
      if (timerRef.current) clearInterval(timerRef.current);
      soundObject?.unloadAsync();
    };
  }, [soundObject]);

  // 녹음 세션을 시작하는 함수
  const startRecording = async () => {
    try {
      const permission = await Audio.requestPermissionsAsync();
      if (permission.status !== 'granted') {
        Alert.alert('권한 필요', '마이크 권한이 필요합니다.');
        return;
      }

      // 녹음 시작 시 오디오 모드 설정
      await Audio.setAudioModeAsync({
        allowsRecordingIOS: true,
        playsInSilentModeIOS: true,
      });

      setIsRecording(true);
      isRecordingRef.current = true;
      await startNewSegment();

      // 타이머 시작 (1초마다)
      timerRef.current = setInterval(() => {
        setRecordingTime(prev => prev + 1);
        setSegmentProgress(prev => (prev + 1) % 5);
      }, 1000);

      // 세그먼트 분할 타이머 시작 (5초마다)
      intervalRef.current = setInterval(handleSegmentSplit, 5000);

    } catch (err) {
      console.error('Failed to start recording', err);
      Alert.alert('녹음 오류', '녹음을 시작할 수 없습니다.');
      setIsRecording(false);
      isRecordingRef.current = false;
    }
  };

  // 녹음 세션을 완전히 중지하는 함수
  const stopRecording = async () => {
    setIsRecording(false); // isRecording 상태를 먼저 false로 설정
    isRecordingRef.current = false;

    // 모든 타이머 정리
    if (intervalRef.current) clearInterval(intervalRef.current);
    if (timerRef.current) clearInterval(timerRef.current);
    intervalRef.current = null;
    timerRef.current = null;
    
    // UI 리셋
    setRecordingTime(0);
    setSegmentProgress(0);

    let finalSegments = [...recordingSegments];

    if (recording) {
      try {
        const savedSegment = await saveCurrentSegment();
        if (savedSegment) {
          finalSegments.push(savedSegment);
        }
        setRecordingSegments(finalSegments); // 최종 세그먼트 목록으로 업데이트
        setRecording(null);
      } catch (err) {
        console.error('Failed to save final segment', err);
      }
    }

    // 녹음 종료 후 오디오 모드 리셋 (선택적)
    await Audio.setAudioModeAsync({
      allowsRecordingIOS: false,
    });
    
    Alert.alert('녹음 완료', `총 ${finalSegments.length}개의 파일이 저장되었습니다.`);
    segmentCountRef.current = 0; // 세그먼트 카운트 리셋
  };
  
  // 새로운 녹음 세그먼트를 시작하는 함수
  const startNewSegment = async () => {
    // 이미 녹음 객체가 있으면 반환
    if (recording) {
      console.log('Recording already in progress, skipping...');
      return;
    }
    
    try {
      // 새 세그먼트 시작 전에 오디오 모드 재설정 (녹음 허용)
      await Audio.setAudioModeAsync({
        allowsRecordingIOS: true,
        playsInSilentModeIOS: true,
      });
      
      console.log('Starting new recording segment...');
      const { recording: newRecording } = await Audio.Recording.createAsync(
        Audio.RecordingOptionsPresets.HIGH_QUALITY
      );
      setRecording(newRecording);
      console.log('New recording segment started successfully');
    } catch (err) {
      console.error('Failed to start new segment', err);
      Alert.alert('녹음 오류', '새로운 녹음 세그먼트를 시작할 수 없습니다.');
      stopRecording(); // 오류 발생 시 녹음 중지
    }
  };

  // 현재 세그먼트를 저장하는 로직 (중복 제거를 위해 분리)
  const saveCurrentSegment = async () => {
    if (!recording) return null;

    try {
      console.log('Stopping and saving current segment...');
      // 현재 녹음 객체를 로컬 변수에 저장
      const currentRecording = recording;
      // 즉시 null로 설정하여 다른 함수에서 접근하지 못하도록 함
      setRecording(null);
      
      await currentRecording.stopAndUnloadAsync();
      
      // 녹음 중지 후 오디오 모드를 비활성화
      await Audio.setAudioModeAsync({
        allowsRecordingIOS: false,
      });
      
      const uri = currentRecording.getURI();

      if (!uri) {
          console.log("URI is null, skipping save.");
          return null;
      }

      const timestamp = new Date().getTime();
      const fileName = `recording_${segmentCountRef.current}_${timestamp}.m4a`;
      const newUri = FileSystem.documentDirectory + fileName;

      await FileSystem.moveAsync({ from: uri, to: newUri });
      console.log(`Segment ${segmentCountRef.current} saved to:`, newUri);

      const newSegment = {
        id: `segment_${segmentCountRef.current}_${timestamp}`,
        uri: newUri,
        fileName,
        timestamp,
      };

      segmentCountRef.current += 1;
      return newSegment;

    } catch (err) {
      console.error('Failed to save segment', err);
      return null;
    }
  };

  // 5초 간격으로 호출되어 세그먼트를 분할하는 함수
  const handleSegmentSplit = async () => {
    if (!recording) return; // 녹음 객체가 없으면 반환
    
    const savedSegment = await saveCurrentSegment();
    if (savedSegment) {
      setRecordingSegments(prev => [...prev, savedSegment]);
    }
    // saveCurrentSegment에서 이미 recording을 null로 설정했으므로 제거
    setSegmentProgress(0);

    // 모드 전환 후 안정성을 위해 지연 추가
    await new Promise(resolve => setTimeout(resolve, 200));

    // isRecordingRef를 사용하여 여전히 녹음 중인지 확인
    if (isRecordingRef.current) {
      await startNewSegment();
    }
  };

  // 녹음/중지 버튼 핸들러
  const handleRecordButtonPress = () => {
    if (isRecording) {
      stopRecording();
    } else {
      startRecording();
    }
  };

  const playSegment = async (segment) => {
    try {
      if (soundObject) {
        await soundObject.unloadAsync();
        setSoundObject(null);
        if (playingSegmentId === segment.id) {
          setPlayingSegmentId(null);
          return;
        }
      }

      const { sound } = await Audio.Sound.createAsync(
        { uri: segment.uri },
        { shouldPlay: true }
      );
      setSoundObject(sound);
      setPlayingSegmentId(segment.id);

      sound.setOnPlaybackStatusUpdate((status) => {
        if (status.didJustFinish) {
          setPlayingSegmentId(null);
          setSoundObject(null);
          sound.unloadAsync();
        }
      });
    } catch (err) {
      console.error('Failed to play audio', err);
      Alert.alert('재생 오류', '오디오를 재생할 수 없습니다.');
    }
  };

  const formatTime = (seconds) => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;
  };

  return (
    <View style={styles.container}>
      <Text style={styles.title}>5초 단위 음성 녹음기</Text>
      
      <TouchableOpacity
        style={[styles.button, isRecording && styles.recordingButton]}
        onPress={handleRecordButtonPress}
        testID="record-button"
      >
        <Text style={styles.buttonText}>
          {isRecording ? '녹음 중지' : '녹음 시작'}
        </Text>
      </TouchableOpacity>

      {isRecording && (
        <View style={styles.recordingInfo}>
          <Text style={styles.recordingText} testID="recording-indicator">
            녹음 중... (5초마다 자동 저장)
          </Text>
          <Text style={styles.timerText} testID="timer-display">
            {formatTime(recordingTime)}
          </Text>
          <View style={styles.progressContainer}>
            <Text style={styles.progressText}>다음 저장까지: {5 - segmentProgress}초</Text>
            <View style={styles.progressBar}>
              <View 
                style={[styles.progressFill, { width: `${(segmentProgress / 5) * 100}%` }]} 
                testID="progress-bar"
              />
            </View>
          </View>
        </View>
      )}

      <ScrollView style={styles.segmentList} contentContainerStyle={styles.segmentContent}>
        <Text style={styles.segmentTitle}>저장된 녹음 파일:</Text>
        <View testID="segments-list">
          {recordingSegments.map((segment, index) => (
            <TouchableOpacity 
              key={segment.id} 
              style={[
                styles.segmentItem,
                playingSegmentId === segment.id && styles.playingSegment
              ]} 
              onPress={() => playSegment(segment)}
              testID={`segment-${index}`}
            >
              <Text style={styles.segmentText}>
                {index + 1}. {segment.fileName}
              </Text>
              <Text style={styles.playText}>
                {playingSegmentId === segment.id ? '재생 중...' : '재생 ▶'}
              </Text>
            </TouchableOpacity>
          ))}
        </View>
      </ScrollView>
    </View>
  );
};

// Styles (기존과 동일)
const styles = StyleSheet.create({
  container: {
    flex: 1,
    alignItems: 'center',
    justifyContent: 'center',
    padding: 20,
    backgroundColor: '#f5f5f5',
  },
  title: {
    fontSize: 24,
    fontWeight: 'bold',
    marginBottom: 30,
    color: '#333',
  },
  button: {
    backgroundColor: '#007AFF',
    paddingHorizontal: 40,
    paddingVertical: 15,
    borderRadius: 10,
    marginBottom: 20,
  },
  recordingButton: {
    backgroundColor: '#FF3B30',
  },
  buttonText: {
    color: 'white',
    fontSize: 18,
    fontWeight: '600',
  },
  recordingInfo: {
    alignItems: 'center',
    marginBottom: 20,
  },
  recordingText: {
    fontSize: 16,
    color: '#FF3B30',
    marginBottom: 10,
  },
  timerText: {
    fontSize: 32,
    fontWeight: 'bold',
    color: '#333',
    marginBottom: 15,
  },
  progressContainer: {
    width: 250,
    alignItems: 'center',
  },
  progressText: {
    fontSize: 14,
    color: '#666',
    marginBottom: 8,
  },
  progressBar: {
    width: '100%',
    height: 8,
    backgroundColor: '#e0e0e0',
    borderRadius: 4,
    overflow: 'hidden',
  },
  progressFill: {
    height: '100%',
    backgroundColor: '#4CAF50',
    borderRadius: 4,
  },
  segmentList: {
    flex: 1,
    width: '100%',
    marginTop: 20,
  },
  segmentContent: {
    alignItems: 'center',
    paddingBottom: 20,
  },
  segmentTitle: {
    fontSize: 18,
    fontWeight: '600',
    marginBottom: 15,
    color: '#333',
  },
  segmentItem: {
    flexDirection: 'row',
    justifyContent: 'space-between',
    alignItems: 'center',
    backgroundColor: 'white',
    padding: 15,
    borderRadius: 8,
    marginVertical: 5,
    width: '90%',
    shadowColor: '#000',
    shadowOffset: { width: 0, height: 1 },
    shadowOpacity: 0.2,
    shadowRadius: 2,
    elevation: 2,
  },
  playingSegment: {
    backgroundColor: '#E3F2FD',
    borderColor: '#2196F3',
    borderWidth: 1,
  },
  segmentText: {
    fontSize: 14,
    color: '#333',
    flex: 1,
  },
  playText: {
    fontSize: 14,
    color: '#007AFF',
    fontWeight: '500',
  },
});

export default AudioRecorder;
</file>

<file path="App.js">
import React from 'react';
import { SafeAreaView, StatusBar, StyleSheet } from 'react-native';
import AudioRecorder from './components/AudioRecorder';

export default function App() {
  return (
    <SafeAreaView style={styles.container}>
      <StatusBar barStyle="dark-content" />
      <AudioRecorder />
    </SafeAreaView>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#fff',
  },
});
</file>

<file path="app.json">
{
  "expo": {
    "name": "expo-test",
    "slug": "expo-test",
    "version": "1.0.0",
    "orientation": "portrait",
    "icon": "./assets/icon.png",
    "userInterfaceStyle": "light",
    "splash": {
      "image": "./assets/splash.png",
      "resizeMode": "contain",
      "backgroundColor": "#ffffff"
    },
    "assetBundlePatterns": [
      "**/*"
    ],
    "ios": {
      "supportsTablet": true,
      "infoPlist": {
        "NSMicrophoneUsageDescription": "This app needs access to microphone to record audio."
      }
    },
    "android": {
      "adaptiveIcon": {
        "foregroundImage": "./assets/adaptive-icon.png",
        "backgroundColor": "#ffffff"
      },
      "permissions": ["RECORD_AUDIO"]
    },
    "web": {
      "favicon": "./assets/favicon.png"
    }
  }
}
</file>

<file path="jest-setup.js">
// Mock expo-av
jest.mock('expo-av', () => ({
  Audio: {
    requestPermissionsAsync: jest.fn(),
    setAudioModeAsync: jest.fn(),
    Recording: {
      createAsync: jest.fn(),
    },
    RecordingOptionsPresets: {
      HIGH_QUALITY: {},
    },
  },
}));

// Mock expo-file-system
jest.mock('expo-file-system', () => ({
  documentDirectory: 'file:///mock/document/directory/',
  moveAsync: jest.fn(),
}));
</file>

<file path="jest.config.js">
module.exports = {
  preset: 'jest-expo',
  transformIgnorePatterns: [
    'node_modules/(?!((jest-)?react-native|@react-native(-community)?)|expo(nent)?|@expo(nent)?/.*|@expo-google-fonts/.*|react-navigation|@react-navigation/.*|@unimodules/.*|unimodules|sentry-expo|native-base|react-native-svg)'
  ],
  moduleFileExtensions: ['ts', 'tsx', 'js', 'jsx'],
  setupFiles: ['<rootDir>/jest-setup.js'],
  setupFilesAfterEnv: ['@testing-library/jest-native/extend-expect'],
  testEnvironment: 'jsdom',
};
</file>

<file path="package.json">
{
  "name": "expo-test",
  "version": "1.0.0",
  "main": "node_modules/expo/AppEntry.js",
  "scripts": {
    "start": "expo start",
    "android": "expo start --android",
    "ios": "expo start --ios",
    "web": "expo start --web",
    "test": "jest",
    "test:watch": "jest --watch"
  },
  "keywords": [],
  "author": "",
  "license": "ISC",
  "description": "",
  "dependencies": {
    "expo": "^53.0.20",
    "expo-av": "^15.1.7",
    "expo-file-system": "^18.1.11",
    "react": "19.0.0",
    "react-native": "0.79.5"
  },
  "devDependencies": {
    "@testing-library/jest-native": "^5.4.3",
    "@testing-library/react-native": "^13.2.2",
    "jest": "^30.0.5",
    "jest-expo": "^53.0.9"
  }
}
</file>

<file path="test-audio.js">
const { Audio } = require('expo-audio');

// Test if Audio module is properly imported
console.log('Audio module:', typeof Audio);
console.log('requestRecordingPermissionsAsync:', typeof Audio.requestRecordingPermissionsAsync);
console.log('setAudioModeAsync:', typeof Audio.setAudioModeAsync);
console.log('Recording:', typeof Audio.Recording);
console.log('RecordingOptionsPresets:', Audio.RecordingOptionsPresets);
</file>

</files>
